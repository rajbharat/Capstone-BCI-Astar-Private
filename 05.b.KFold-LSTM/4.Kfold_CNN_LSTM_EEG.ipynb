{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4.Kfold_CNN_LSTM_EEG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WSLTKGWkYbbG",
        "colab": {}
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6e29FA4SYbd0",
        "outputId": "45ed4b68-fc63-4d9b-c775-f0c6f11176fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!kaggle competitions download -c grasp-and-lift-eeg-detection"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading train.zip to /content\n",
            "100% 913M/915M [00:14<00:00, 101MB/s] \n",
            "100% 915M/915M [00:14<00:00, 67.7MB/s]\n",
            "Downloading test.zip to /content\n",
            " 99% 151M/153M [00:01<00:00, 124MB/s]\n",
            "100% 153M/153M [00:01<00:00, 132MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/5.16M [00:00<?, ?B/s]\n",
            "100% 5.16M/5.16M [00:00<00:00, 47.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nly8zqezYbgV",
        "outputId": "ec778661-8612-4e65-e358-a93099aa1243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!unzip train.zip\n",
        "!unzip test.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "   creating: train/\n",
            "  inflating: train/subj10_series1_data.csv  \n",
            "  inflating: train/subj10_series1_events.csv  \n",
            "  inflating: train/subj10_series2_data.csv  \n",
            "  inflating: train/subj10_series2_events.csv  \n",
            "  inflating: train/subj10_series3_data.csv  \n",
            "  inflating: train/subj10_series3_events.csv  \n",
            "  inflating: train/subj10_series4_data.csv  \n",
            "  inflating: train/subj10_series4_events.csv  \n",
            "  inflating: train/subj10_series5_data.csv  \n",
            "  inflating: train/subj10_series5_events.csv  \n",
            "  inflating: train/subj10_series6_data.csv  \n",
            "  inflating: train/subj10_series6_events.csv  \n",
            "  inflating: train/subj10_series7_data.csv  \n",
            "  inflating: train/subj10_series7_events.csv  \n",
            "  inflating: train/subj10_series8_data.csv  \n",
            "  inflating: train/subj10_series8_events.csv  \n",
            "  inflating: train/subj11_series1_data.csv  \n",
            "  inflating: train/subj11_series1_events.csv  \n",
            "  inflating: train/subj11_series2_data.csv  \n",
            "  inflating: train/subj11_series2_events.csv  \n",
            "  inflating: train/subj11_series3_data.csv  \n",
            "  inflating: train/subj11_series3_events.csv  \n",
            "  inflating: train/subj11_series4_data.csv  \n",
            "  inflating: train/subj11_series4_events.csv  \n",
            "  inflating: train/subj11_series5_data.csv  \n",
            "  inflating: train/subj11_series5_events.csv  \n",
            "  inflating: train/subj11_series6_data.csv  \n",
            "  inflating: train/subj11_series6_events.csv  \n",
            "  inflating: train/subj11_series7_data.csv  \n",
            "  inflating: train/subj11_series7_events.csv  \n",
            "  inflating: train/subj11_series8_data.csv  \n",
            "  inflating: train/subj11_series8_events.csv  \n",
            "  inflating: train/subj12_series1_data.csv  \n",
            "  inflating: train/subj12_series1_events.csv  \n",
            "  inflating: train/subj12_series2_data.csv  \n",
            "  inflating: train/subj12_series2_events.csv  \n",
            "  inflating: train/subj12_series3_data.csv  \n",
            "  inflating: train/subj12_series3_events.csv  \n",
            "  inflating: train/subj12_series4_data.csv  \n",
            "  inflating: train/subj12_series4_events.csv  \n",
            "  inflating: train/subj12_series5_data.csv  \n",
            "  inflating: train/subj12_series5_events.csv  \n",
            "  inflating: train/subj12_series6_data.csv  \n",
            "  inflating: train/subj12_series6_events.csv  \n",
            "  inflating: train/subj12_series7_data.csv  \n",
            "  inflating: train/subj12_series7_events.csv  \n",
            "  inflating: train/subj12_series8_data.csv  \n",
            "  inflating: train/subj12_series8_events.csv  \n",
            "  inflating: train/subj1_series1_data.csv  \n",
            "  inflating: train/subj1_series1_events.csv  \n",
            "  inflating: train/subj1_series2_data.csv  \n",
            "  inflating: train/subj1_series2_events.csv  \n",
            "  inflating: train/subj1_series3_data.csv  \n",
            "  inflating: train/subj1_series3_events.csv  \n",
            "  inflating: train/subj1_series4_data.csv  \n",
            "  inflating: train/subj1_series4_events.csv  \n",
            "  inflating: train/subj1_series5_data.csv  \n",
            "  inflating: train/subj1_series5_events.csv  \n",
            "  inflating: train/subj1_series6_data.csv  \n",
            "  inflating: train/subj1_series6_events.csv  \n",
            "  inflating: train/subj1_series7_data.csv  \n",
            "  inflating: train/subj1_series7_events.csv  \n",
            "  inflating: train/subj1_series8_data.csv  \n",
            "  inflating: train/subj1_series8_events.csv  \n",
            "  inflating: train/subj2_series1_data.csv  \n",
            "  inflating: train/subj2_series1_events.csv  \n",
            "  inflating: train/subj2_series2_data.csv  \n",
            "  inflating: train/subj2_series2_events.csv  \n",
            "  inflating: train/subj2_series3_data.csv  \n",
            "  inflating: train/subj2_series3_events.csv  \n",
            "  inflating: train/subj2_series4_data.csv  \n",
            "  inflating: train/subj2_series4_events.csv  \n",
            "  inflating: train/subj2_series5_data.csv  \n",
            "  inflating: train/subj2_series5_events.csv  \n",
            "  inflating: train/subj2_series6_data.csv  \n",
            "  inflating: train/subj2_series6_events.csv  \n",
            "  inflating: train/subj2_series7_data.csv  \n",
            "  inflating: train/subj2_series7_events.csv  \n",
            "  inflating: train/subj2_series8_data.csv  \n",
            "  inflating: train/subj2_series8_events.csv  \n",
            "  inflating: train/subj3_series1_data.csv  \n",
            "  inflating: train/subj3_series1_events.csv  \n",
            "  inflating: train/subj3_series2_data.csv  \n",
            "  inflating: train/subj3_series2_events.csv  \n",
            "  inflating: train/subj3_series3_data.csv  \n",
            "  inflating: train/subj3_series3_events.csv  \n",
            "  inflating: train/subj3_series4_data.csv  \n",
            "  inflating: train/subj3_series4_events.csv  \n",
            "  inflating: train/subj3_series5_data.csv  \n",
            "  inflating: train/subj3_series5_events.csv  \n",
            "  inflating: train/subj3_series6_data.csv  \n",
            "  inflating: train/subj3_series6_events.csv  \n",
            "  inflating: train/subj3_series7_data.csv  \n",
            "  inflating: train/subj3_series7_events.csv  \n",
            "  inflating: train/subj3_series8_data.csv  \n",
            "  inflating: train/subj3_series8_events.csv  \n",
            "  inflating: train/subj4_series1_data.csv  \n",
            "  inflating: train/subj4_series1_events.csv  \n",
            "  inflating: train/subj4_series2_data.csv  \n",
            "  inflating: train/subj4_series2_events.csv  \n",
            "  inflating: train/subj4_series3_data.csv  \n",
            "  inflating: train/subj4_series3_events.csv  \n",
            "  inflating: train/subj4_series4_data.csv  \n",
            "  inflating: train/subj4_series4_events.csv  \n",
            "  inflating: train/subj4_series5_data.csv  \n",
            "  inflating: train/subj4_series5_events.csv  \n",
            "  inflating: train/subj4_series6_data.csv  \n",
            "  inflating: train/subj4_series6_events.csv  \n",
            "  inflating: train/subj4_series7_data.csv  \n",
            "  inflating: train/subj4_series7_events.csv  \n",
            "  inflating: train/subj4_series8_data.csv  \n",
            "  inflating: train/subj4_series8_events.csv  \n",
            "  inflating: train/subj5_series1_data.csv  \n",
            "  inflating: train/subj5_series1_events.csv  \n",
            "  inflating: train/subj5_series2_data.csv  \n",
            "  inflating: train/subj5_series2_events.csv  \n",
            "  inflating: train/subj5_series3_data.csv  \n",
            "  inflating: train/subj5_series3_events.csv  \n",
            "  inflating: train/subj5_series4_data.csv  \n",
            "  inflating: train/subj5_series4_events.csv  \n",
            "  inflating: train/subj5_series5_data.csv  \n",
            "  inflating: train/subj5_series5_events.csv  \n",
            "  inflating: train/subj5_series6_data.csv  \n",
            "  inflating: train/subj5_series6_events.csv  \n",
            "  inflating: train/subj5_series7_data.csv  \n",
            "  inflating: train/subj5_series7_events.csv  \n",
            "  inflating: train/subj5_series8_data.csv  \n",
            "  inflating: train/subj5_series8_events.csv  \n",
            "  inflating: train/subj6_series1_data.csv  \n",
            "  inflating: train/subj6_series1_events.csv  \n",
            "  inflating: train/subj6_series2_data.csv  \n",
            "  inflating: train/subj6_series2_events.csv  \n",
            "  inflating: train/subj6_series3_data.csv  \n",
            "  inflating: train/subj6_series3_events.csv  \n",
            "  inflating: train/subj6_series4_data.csv  \n",
            "  inflating: train/subj6_series4_events.csv  \n",
            "  inflating: train/subj6_series5_data.csv  \n",
            "  inflating: train/subj6_series5_events.csv  \n",
            "  inflating: train/subj6_series6_data.csv  \n",
            "  inflating: train/subj6_series6_events.csv  \n",
            "  inflating: train/subj6_series7_data.csv  \n",
            "  inflating: train/subj6_series7_events.csv  \n",
            "  inflating: train/subj6_series8_data.csv  \n",
            "  inflating: train/subj6_series8_events.csv  \n",
            "  inflating: train/subj7_series1_data.csv  \n",
            "  inflating: train/subj7_series1_events.csv  \n",
            "  inflating: train/subj7_series2_data.csv  \n",
            "  inflating: train/subj7_series2_events.csv  \n",
            "  inflating: train/subj7_series3_data.csv  \n",
            "  inflating: train/subj7_series3_events.csv  \n",
            "  inflating: train/subj7_series4_data.csv  \n",
            "  inflating: train/subj7_series4_events.csv  \n",
            "  inflating: train/subj7_series5_data.csv  \n",
            "  inflating: train/subj7_series5_events.csv  \n",
            "  inflating: train/subj7_series6_data.csv  \n",
            "  inflating: train/subj7_series6_events.csv  \n",
            "  inflating: train/subj7_series7_data.csv  \n",
            "  inflating: train/subj7_series7_events.csv  \n",
            "  inflating: train/subj7_series8_data.csv  \n",
            "  inflating: train/subj7_series8_events.csv  \n",
            "  inflating: train/subj8_series1_data.csv  \n",
            "  inflating: train/subj8_series1_events.csv  \n",
            "  inflating: train/subj8_series2_data.csv  \n",
            "  inflating: train/subj8_series2_events.csv  \n",
            "  inflating: train/subj8_series3_data.csv  \n",
            "  inflating: train/subj8_series3_events.csv  \n",
            "  inflating: train/subj8_series4_data.csv  \n",
            "  inflating: train/subj8_series4_events.csv  \n",
            "  inflating: train/subj8_series5_data.csv  \n",
            "  inflating: train/subj8_series5_events.csv  \n",
            "  inflating: train/subj8_series6_data.csv  \n",
            "  inflating: train/subj8_series6_events.csv  \n",
            "  inflating: train/subj8_series7_data.csv  \n",
            "  inflating: train/subj8_series7_events.csv  \n",
            "  inflating: train/subj8_series8_data.csv  \n",
            "  inflating: train/subj8_series8_events.csv  \n",
            "  inflating: train/subj9_series1_data.csv  \n",
            "  inflating: train/subj9_series1_events.csv  \n",
            "  inflating: train/subj9_series2_data.csv  \n",
            "  inflating: train/subj9_series2_events.csv  \n",
            "  inflating: train/subj9_series3_data.csv  \n",
            "  inflating: train/subj9_series3_events.csv  \n",
            "  inflating: train/subj9_series4_data.csv  \n",
            "  inflating: train/subj9_series4_events.csv  \n",
            "  inflating: train/subj9_series5_data.csv  \n",
            "  inflating: train/subj9_series5_events.csv  \n",
            "  inflating: train/subj9_series6_data.csv  \n",
            "  inflating: train/subj9_series6_events.csv  \n",
            "  inflating: train/subj9_series7_data.csv  \n",
            "  inflating: train/subj9_series7_events.csv  \n",
            "  inflating: train/subj9_series8_data.csv  \n",
            "  inflating: train/subj9_series8_events.csv  \n",
            "Archive:  test.zip\n",
            "   creating: test/\n",
            "  inflating: test/subj10_series10_data.csv  \n",
            "  inflating: test/subj10_series9_data.csv  \n",
            "  inflating: test/subj11_series10_data.csv  \n",
            "  inflating: test/subj11_series9_data.csv  \n",
            "  inflating: test/subj12_series10_data.csv  \n",
            "  inflating: test/subj12_series9_data.csv  \n",
            "  inflating: test/subj1_series10_data.csv  \n",
            "  inflating: test/subj1_series9_data.csv  \n",
            "  inflating: test/subj2_series10_data.csv  \n",
            "  inflating: test/subj2_series9_data.csv  \n",
            "  inflating: test/subj3_series10_data.csv  \n",
            "  inflating: test/subj3_series9_data.csv  \n",
            "  inflating: test/subj4_series10_data.csv  \n",
            "  inflating: test/subj4_series9_data.csv  \n",
            "  inflating: test/subj5_series10_data.csv  \n",
            "  inflating: test/subj5_series9_data.csv  \n",
            "  inflating: test/subj6_series10_data.csv  \n",
            "  inflating: test/subj6_series9_data.csv  \n",
            "  inflating: test/subj7_series10_data.csv  \n",
            "  inflating: test/subj7_series9_data.csv  \n",
            "  inflating: test/subj8_series10_data.csv  \n",
            "  inflating: test/subj8_series9_data.csv  \n",
            "  inflating: test/subj9_series10_data.csv  \n",
            "  inflating: test/subj9_series9_data.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TgdT0K-iYbi3",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score as auc\n",
        "from scipy.interpolate import BSpline\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gV0QGLTrYblZ",
        "colab": {}
      },
      "source": [
        "USE_CUDA = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9mqmqaRWYbnv",
        "colab": {}
      },
      "source": [
        "def prepare_data_train(fname):\n",
        "    \n",
        "    data = pd.read_csv(fname)\n",
        "    \n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    \n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "\n",
        "\n",
        "scaler= StandardScaler()\n",
        "def data_preprocess_train(X):\n",
        "    X_prep=scaler.fit_transform(X)\n",
        "    \n",
        "    return X_prep\n",
        "def data_preprocess_test(X):\n",
        "    X_prep=scaler.transform(X)\n",
        "    \n",
        "    return X_prep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lGUvW7Jfyt9B",
        "colab": {}
      },
      "source": [
        "def load_data(subjects,series):\n",
        "  y_raw= []\n",
        "  raw = []\n",
        "  for subject in subjects:\n",
        "    for ser in series:\n",
        "      fname ='train/subj%d_series%d_data.csv' % (subject,ser)\n",
        "      # print(fname)\n",
        "      data,labels=prepare_data_train(fname)\n",
        "      raw.append(data)\n",
        "      y_raw.append(labels)\n",
        "\n",
        "    X = pd.concat(raw)\n",
        "    y = pd.concat(y_raw)\n",
        "      \n",
        "    X =np.asarray(X.astype(float))\n",
        "    y = np.asarray(y.astype(float))\n",
        "\n",
        "  return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vSGbGQh-y88B",
        "colab": {}
      },
      "source": [
        "# some parameteres for the model\n",
        "num_features = 32\n",
        "window_size = 1024\n",
        "batch_size=2000\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6yp8IaygD8f",
        "colab": {}
      },
      "source": [
        "def get_batch(dataset,target, batch_size=2000, val=False, index=None):\n",
        "    if val == False:\n",
        "        index = random.randint(window_size, len(dataset) - 16 * batch_size)\n",
        "        indexes = np.arange(index, index + 16*batch_size, 16)\n",
        "\n",
        "    else:\n",
        "        indexes = np.arange(index, index + batch_size)\n",
        "    \n",
        "    batch = np.zeros((batch_size, num_features, window_size//4))\n",
        "    \n",
        "    b = 0\n",
        "    for i in indexes:\n",
        "        \n",
        "        start = i - window_size if i - window_size > 0 else 0\n",
        "        \n",
        "        tmp = dataset[start:i]\n",
        "        batch[b,:,:] = tmp[::4].transpose()\n",
        "        \n",
        "        b += 1\n",
        "\n",
        "    targets = target[indexes]\n",
        "    return torch.DoubleTensor(batch), torch.DoubleTensor(targets) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pd-3dbRAgD6R",
        "colab": {}
      },
      "source": [
        "class convmodel(nn.Module):\n",
        "  def __init__(self, drop=0.5, d_linear=124):\n",
        "    super().__init__()\n",
        "    self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=0, stride=1)\n",
        "    self.bn = nn.BatchNorm1d(64)\n",
        "    self.pool = nn.MaxPool1d(2, stride=2)\n",
        "    \n",
        "    self.dropout1 = nn.Dropout(drop)\n",
        " \n",
        "    self.conv = nn.Sequential(self.conv2, nn.ReLU(inplace=True), self.bn,self.pool, self.dropout1)\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return x\n",
        "\n",
        "class Combine(nn.Module):\n",
        "    def __init__(self,out_classes):\n",
        "        super(Combine, self).__init__()\n",
        "        self.cnn = convmodel().double()\n",
        "        self.rnn = nn.LSTM(input_size=127, hidden_size=64, num_layers=1,batch_first=True)\n",
        "        self.linear = nn.Linear(64,out_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.cnn(x)\n",
        "   \n",
        "      out, hidden=self.rnn(x)\n",
        "\n",
        "      out = self.linear(out[:, -1, :])\n",
        "\n",
        "      return torch.sigmoid(out)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tm9puItwgLh5",
        "colab": {}
      },
      "source": [
        "model = Combine(6).double()\n",
        "if USE_CUDA == 1:\n",
        "    model = model.cuda()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtKK5KVLgQA4",
        "colab": {}
      },
      "source": [
        "bs = batch_size\n",
        "def train(traindata,y_train, epochs, printevery=100, shuffle=True):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(traindata)//bs):\n",
        "            optim.zero_grad()\n",
        "            x, y = get_batch(traindata,y_train)\n",
        "            if USE_CUDA == 1:\n",
        "                x = Variable(x).cuda()\n",
        "                y = Variable(y).cuda()\n",
        "            preds = model(x)\n",
        "            loss = F.binary_cross_entropy(preds.view(-1), y.view(-1))\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            optim.step()\n",
        "            if (i + 1) % printevery == 0:\n",
        "                print(\"epoch: %d, iter %d/%d, loss %.4f\"%(epoch + 1, i + 1, len(traindata)//2000, total_loss/printevery))\n",
        "                total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w6NzCniYglyV",
        "colab": {}
      },
      "source": [
        "def getPredictions(data,labels):\n",
        "    model.eval()\n",
        "    p = []\n",
        "    res = []\n",
        "    i = window_size\n",
        "    bs = 2000\n",
        "    while i < len(data):\n",
        "        if i + bs > len(data):\n",
        "            bs = len(data) - i\n",
        "        x, y = get_batch(data,labels, bs, index=i, val=True)\n",
        "        x = (x)\n",
        "        x = x.cuda()\n",
        "        y = y\n",
        "        preds = model(x)\n",
        "        preds = preds.squeeze(1)\n",
        "        p.append(np.array(preds.cpu().data))\n",
        "        res.append(np.array(y.data))\n",
        "        i += bs\n",
        "    preds = p[0]\n",
        "    for i in p[1:]:\n",
        "        preds = np.vstack((preds,i))\n",
        "\n",
        "    targs = res[0]\n",
        "    for i in res[1:]:\n",
        "        targs = np.vstack((targs, i))\n",
        "    return preds, targs\n",
        "\n",
        "def valscore(preds, targs):\n",
        "    aucs = [auc(targs[:, j], preds[:, j]) for j in range(6)]\n",
        "    total_loss = np.mean(aucs)\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def accurecy(preds, targs):\n",
        "  preds=np.where(preds>0.3,1,0)\n",
        "  acc_score=[]\n",
        "  for j in range(6):\n",
        "    acc_score.append(accuracy_score(targs[:, j],preds[:, j]))\n",
        "  return np.mean(acc_score)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1g_wSqCvYbtZ",
        "outputId": "78c648fc-a5b7-4c80-e4c4-decb54d70ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "subjects=range(1,13)\n",
        "\n",
        "\n",
        "X=np.array([1,2,3,4,5,6,7,8])\n",
        "kf = KFold(n_splits=4,shuffle=True)\n",
        "count=1\n",
        "auc_score=[]\n",
        "for train_index, test_index in kf.split(X):\n",
        "  print(\"Fold \",count)\n",
        "  print(\"train\",X[train_index],\"test\",X[test_index])\n",
        "  X_train,y_train=load_data(subjects,series =X[train_index])\n",
        "  X_train=data_preprocess_train(X_train)\n",
        "  \n",
        "  X_test,y_test=load_data(subjects,series=X[test_index])\n",
        "  X_test=data_preprocess_test(X_test)\n",
        "  train(X_train,y_train,1)\n",
        "  val_preds, val_targs=getPredictions(X_test,y_test)\n",
        "  print(\"check results\")\n",
        "  print(valscore(val_preds, val_targs))\n",
        "  auc_score.append(valscore(val_preds, val_targs))\n",
        "  with open(\"results.txt\", \"a\") as res_file:\n",
        "    res_file.write(\"train : \"+str(X[train_index])+\" test : \"+str(X[test_index])+\" AUC score : \"+str(valscore(val_preds, val_targs))+\" Accurecy Score: \"+str(accurecy(val_preds, val_targs))+\"\\n\")\n",
        "  count+=1\n",
        "print(np.mean(auc_score))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold  1\n",
            "train [1 2 3 5 7 8] test [4 6]\n",
            "epoch: 1, iter 100/6590, loss 0.2581\n",
            "epoch: 1, iter 200/6590, loss 0.1192\n",
            "epoch: 1, iter 300/6590, loss 0.1160\n",
            "epoch: 1, iter 400/6590, loss 0.1195\n",
            "epoch: 1, iter 500/6590, loss 0.1176\n",
            "epoch: 1, iter 600/6590, loss 0.1138\n",
            "epoch: 1, iter 700/6590, loss 0.1111\n",
            "epoch: 1, iter 800/6590, loss 0.1087\n",
            "epoch: 1, iter 900/6590, loss 0.1051\n",
            "epoch: 1, iter 1000/6590, loss 0.0986\n",
            "epoch: 1, iter 1100/6590, loss 0.1034\n",
            "epoch: 1, iter 1200/6590, loss 0.0978\n",
            "epoch: 1, iter 1300/6590, loss 0.0995\n",
            "epoch: 1, iter 1400/6590, loss 0.1000\n",
            "epoch: 1, iter 1500/6590, loss 0.0973\n",
            "epoch: 1, iter 1600/6590, loss 0.1007\n",
            "epoch: 1, iter 1700/6590, loss 0.1029\n",
            "epoch: 1, iter 1800/6590, loss 0.0995\n",
            "epoch: 1, iter 1900/6590, loss 0.0989\n",
            "epoch: 1, iter 2000/6590, loss 0.1011\n",
            "epoch: 1, iter 2100/6590, loss 0.0950\n",
            "epoch: 1, iter 2200/6590, loss 0.0958\n",
            "epoch: 1, iter 2300/6590, loss 0.0934\n",
            "epoch: 1, iter 2400/6590, loss 0.1000\n",
            "epoch: 1, iter 2500/6590, loss 0.0961\n",
            "epoch: 1, iter 2600/6590, loss 0.0967\n",
            "epoch: 1, iter 2700/6590, loss 0.0963\n",
            "epoch: 1, iter 2800/6590, loss 0.0921\n",
            "epoch: 1, iter 2900/6590, loss 0.0936\n",
            "epoch: 1, iter 3000/6590, loss 0.0976\n",
            "epoch: 1, iter 3100/6590, loss 0.0910\n",
            "epoch: 1, iter 3200/6590, loss 0.0941\n",
            "epoch: 1, iter 3300/6590, loss 0.0946\n",
            "epoch: 1, iter 3400/6590, loss 0.0909\n",
            "epoch: 1, iter 3500/6590, loss 0.0945\n",
            "epoch: 1, iter 3600/6590, loss 0.0925\n",
            "epoch: 1, iter 3700/6590, loss 0.0936\n",
            "epoch: 1, iter 3800/6590, loss 0.0960\n",
            "epoch: 1, iter 3900/6590, loss 0.0902\n",
            "epoch: 1, iter 4000/6590, loss 0.0957\n",
            "epoch: 1, iter 4100/6590, loss 0.0885\n",
            "epoch: 1, iter 4200/6590, loss 0.0877\n",
            "epoch: 1, iter 4300/6590, loss 0.0913\n",
            "epoch: 1, iter 4400/6590, loss 0.0842\n",
            "epoch: 1, iter 4500/6590, loss 0.0932\n",
            "epoch: 1, iter 4600/6590, loss 0.0880\n",
            "epoch: 1, iter 4700/6590, loss 0.0879\n",
            "epoch: 1, iter 4800/6590, loss 0.0941\n",
            "epoch: 1, iter 4900/6590, loss 0.0927\n",
            "epoch: 1, iter 5000/6590, loss 0.0865\n",
            "epoch: 1, iter 5100/6590, loss 0.0892\n",
            "epoch: 1, iter 5200/6590, loss 0.0812\n",
            "epoch: 1, iter 5300/6590, loss 0.0874\n",
            "epoch: 1, iter 5400/6590, loss 0.0908\n",
            "epoch: 1, iter 5500/6590, loss 0.0859\n",
            "epoch: 1, iter 5600/6590, loss 0.0838\n",
            "epoch: 1, iter 5700/6590, loss 0.0878\n",
            "epoch: 1, iter 5800/6590, loss 0.0929\n",
            "epoch: 1, iter 5900/6590, loss 0.0858\n",
            "epoch: 1, iter 6000/6590, loss 0.0895\n",
            "epoch: 1, iter 6100/6590, loss 0.0891\n",
            "epoch: 1, iter 6200/6590, loss 0.0853\n",
            "epoch: 1, iter 6300/6590, loss 0.0845\n",
            "epoch: 1, iter 6400/6590, loss 0.0846\n",
            "epoch: 1, iter 6500/6590, loss 0.0818\n",
            "check results\n",
            "0.8934568187485915\n",
            "Fold  2\n",
            "train [1 2 4 5 6 7] test [3 8]\n",
            "epoch: 1, iter 100/6978, loss 0.0854\n",
            "epoch: 1, iter 200/6978, loss 0.0807\n",
            "epoch: 1, iter 300/6978, loss 0.0792\n",
            "epoch: 1, iter 400/6978, loss 0.0784\n",
            "epoch: 1, iter 500/6978, loss 0.0843\n",
            "epoch: 1, iter 600/6978, loss 0.0763\n",
            "epoch: 1, iter 700/6978, loss 0.0789\n",
            "epoch: 1, iter 800/6978, loss 0.0784\n",
            "epoch: 1, iter 900/6978, loss 0.0739\n",
            "epoch: 1, iter 1000/6978, loss 0.0817\n",
            "epoch: 1, iter 1100/6978, loss 0.0810\n",
            "epoch: 1, iter 1200/6978, loss 0.0740\n",
            "epoch: 1, iter 1300/6978, loss 0.0808\n",
            "epoch: 1, iter 1400/6978, loss 0.0807\n",
            "epoch: 1, iter 1500/6978, loss 0.0853\n",
            "epoch: 1, iter 1600/6978, loss 0.0798\n",
            "epoch: 1, iter 1700/6978, loss 0.0830\n",
            "epoch: 1, iter 1800/6978, loss 0.0756\n",
            "epoch: 1, iter 1900/6978, loss 0.0817\n",
            "epoch: 1, iter 2000/6978, loss 0.0830\n",
            "epoch: 1, iter 2100/6978, loss 0.0813\n",
            "epoch: 1, iter 2200/6978, loss 0.0763\n",
            "epoch: 1, iter 2300/6978, loss 0.0766\n",
            "epoch: 1, iter 2400/6978, loss 0.0761\n",
            "epoch: 1, iter 2500/6978, loss 0.0812\n",
            "epoch: 1, iter 2600/6978, loss 0.0806\n",
            "epoch: 1, iter 2700/6978, loss 0.0773\n",
            "epoch: 1, iter 2800/6978, loss 0.0788\n",
            "epoch: 1, iter 2900/6978, loss 0.0820\n",
            "epoch: 1, iter 3000/6978, loss 0.0830\n",
            "epoch: 1, iter 3100/6978, loss 0.0758\n",
            "epoch: 1, iter 3200/6978, loss 0.0760\n",
            "epoch: 1, iter 3300/6978, loss 0.0755\n",
            "epoch: 1, iter 3400/6978, loss 0.0782\n",
            "epoch: 1, iter 3500/6978, loss 0.0777\n",
            "epoch: 1, iter 3600/6978, loss 0.0740\n",
            "epoch: 1, iter 3700/6978, loss 0.0762\n",
            "epoch: 1, iter 3800/6978, loss 0.0763\n",
            "epoch: 1, iter 3900/6978, loss 0.0777\n",
            "epoch: 1, iter 4000/6978, loss 0.0755\n",
            "epoch: 1, iter 4100/6978, loss 0.0787\n",
            "epoch: 1, iter 4200/6978, loss 0.0738\n",
            "epoch: 1, iter 4300/6978, loss 0.0742\n",
            "epoch: 1, iter 4400/6978, loss 0.0740\n",
            "epoch: 1, iter 4500/6978, loss 0.0745\n",
            "epoch: 1, iter 4600/6978, loss 0.0799\n",
            "epoch: 1, iter 4700/6978, loss 0.0769\n",
            "epoch: 1, iter 4800/6978, loss 0.0785\n",
            "epoch: 1, iter 4900/6978, loss 0.0762\n",
            "epoch: 1, iter 5000/6978, loss 0.0787\n",
            "epoch: 1, iter 5100/6978, loss 0.0783\n",
            "epoch: 1, iter 5200/6978, loss 0.0732\n",
            "epoch: 1, iter 5300/6978, loss 0.0777\n",
            "epoch: 1, iter 5400/6978, loss 0.0723\n",
            "epoch: 1, iter 5500/6978, loss 0.0782\n",
            "epoch: 1, iter 5600/6978, loss 0.0721\n",
            "epoch: 1, iter 5700/6978, loss 0.0768\n",
            "epoch: 1, iter 5800/6978, loss 0.0711\n",
            "epoch: 1, iter 5900/6978, loss 0.0709\n",
            "epoch: 1, iter 6000/6978, loss 0.0749\n",
            "epoch: 1, iter 6100/6978, loss 0.0777\n",
            "epoch: 1, iter 6200/6978, loss 0.0746\n",
            "epoch: 1, iter 6300/6978, loss 0.0694\n",
            "epoch: 1, iter 6400/6978, loss 0.0799\n",
            "epoch: 1, iter 6500/6978, loss 0.0761\n",
            "epoch: 1, iter 6600/6978, loss 0.0728\n",
            "epoch: 1, iter 6700/6978, loss 0.0705\n",
            "epoch: 1, iter 6800/6978, loss 0.0772\n",
            "epoch: 1, iter 6900/6978, loss 0.0713\n",
            "check results\n",
            "0.8639678707344066\n",
            "Fold  3\n",
            "train [1 3 4 6 7 8] test [2 5]\n",
            "epoch: 1, iter 100/6433, loss 0.0800\n",
            "epoch: 1, iter 200/6433, loss 0.0789\n",
            "epoch: 1, iter 300/6433, loss 0.0801\n",
            "epoch: 1, iter 400/6433, loss 0.0770\n",
            "epoch: 1, iter 500/6433, loss 0.0756\n",
            "epoch: 1, iter 600/6433, loss 0.0801\n",
            "epoch: 1, iter 700/6433, loss 0.0829\n",
            "epoch: 1, iter 800/6433, loss 0.0855\n",
            "epoch: 1, iter 900/6433, loss 0.0788\n",
            "epoch: 1, iter 1000/6433, loss 0.0811\n",
            "epoch: 1, iter 1100/6433, loss 0.0790\n",
            "epoch: 1, iter 1200/6433, loss 0.0851\n",
            "epoch: 1, iter 1300/6433, loss 0.0786\n",
            "epoch: 1, iter 1400/6433, loss 0.0815\n",
            "epoch: 1, iter 1500/6433, loss 0.0810\n",
            "epoch: 1, iter 1600/6433, loss 0.0783\n",
            "epoch: 1, iter 1700/6433, loss 0.0796\n",
            "epoch: 1, iter 1800/6433, loss 0.0786\n",
            "epoch: 1, iter 1900/6433, loss 0.0771\n",
            "epoch: 1, iter 2000/6433, loss 0.0792\n",
            "epoch: 1, iter 2100/6433, loss 0.0822\n",
            "epoch: 1, iter 2200/6433, loss 0.0832\n",
            "epoch: 1, iter 2300/6433, loss 0.0745\n",
            "epoch: 1, iter 2400/6433, loss 0.0765\n",
            "epoch: 1, iter 2500/6433, loss 0.0823\n",
            "epoch: 1, iter 2600/6433, loss 0.0755\n",
            "epoch: 1, iter 2700/6433, loss 0.0727\n",
            "epoch: 1, iter 2800/6433, loss 0.0769\n",
            "epoch: 1, iter 2900/6433, loss 0.0765\n",
            "epoch: 1, iter 3000/6433, loss 0.0766\n",
            "epoch: 1, iter 3100/6433, loss 0.0772\n",
            "epoch: 1, iter 3200/6433, loss 0.0842\n",
            "epoch: 1, iter 3300/6433, loss 0.0768\n",
            "epoch: 1, iter 3400/6433, loss 0.0709\n",
            "epoch: 1, iter 3500/6433, loss 0.0793\n",
            "epoch: 1, iter 3600/6433, loss 0.0785\n",
            "epoch: 1, iter 3700/6433, loss 0.0760\n",
            "epoch: 1, iter 3800/6433, loss 0.0843\n",
            "epoch: 1, iter 3900/6433, loss 0.0798\n",
            "epoch: 1, iter 4000/6433, loss 0.0752\n",
            "epoch: 1, iter 4100/6433, loss 0.0755\n",
            "epoch: 1, iter 4200/6433, loss 0.0781\n",
            "epoch: 1, iter 4300/6433, loss 0.0800\n",
            "epoch: 1, iter 4400/6433, loss 0.0786\n",
            "epoch: 1, iter 4500/6433, loss 0.0785\n",
            "epoch: 1, iter 4600/6433, loss 0.0745\n",
            "epoch: 1, iter 4700/6433, loss 0.0810\n",
            "epoch: 1, iter 4800/6433, loss 0.0773\n",
            "epoch: 1, iter 4900/6433, loss 0.0747\n",
            "epoch: 1, iter 5000/6433, loss 0.0792\n",
            "epoch: 1, iter 5100/6433, loss 0.0790\n",
            "epoch: 1, iter 5200/6433, loss 0.0789\n",
            "epoch: 1, iter 5300/6433, loss 0.0758\n",
            "epoch: 1, iter 5400/6433, loss 0.0756\n",
            "epoch: 1, iter 5500/6433, loss 0.0775\n",
            "epoch: 1, iter 5600/6433, loss 0.0757\n",
            "epoch: 1, iter 5700/6433, loss 0.0715\n",
            "epoch: 1, iter 5800/6433, loss 0.0756\n",
            "epoch: 1, iter 5900/6433, loss 0.0786\n",
            "epoch: 1, iter 6000/6433, loss 0.0781\n",
            "epoch: 1, iter 6100/6433, loss 0.0765\n",
            "epoch: 1, iter 6200/6433, loss 0.0773\n",
            "epoch: 1, iter 6300/6433, loss 0.0805\n",
            "epoch: 1, iter 6400/6433, loss 0.0739\n",
            "check results\n",
            "0.8783394949979492\n",
            "Fold  4\n",
            "train [2 3 4 5 6 8] test [1 7]\n",
            "epoch: 1, iter 100/6976, loss 0.0717\n",
            "epoch: 1, iter 200/6976, loss 0.0716\n",
            "epoch: 1, iter 300/6976, loss 0.0727\n",
            "epoch: 1, iter 400/6976, loss 0.0726\n",
            "epoch: 1, iter 500/6976, loss 0.0734\n",
            "epoch: 1, iter 600/6976, loss 0.0717\n",
            "epoch: 1, iter 700/6976, loss 0.0695\n",
            "epoch: 1, iter 800/6976, loss 0.0714\n",
            "epoch: 1, iter 900/6976, loss 0.0692\n",
            "epoch: 1, iter 1000/6976, loss 0.0708\n",
            "epoch: 1, iter 1100/6976, loss 0.0688\n",
            "epoch: 1, iter 1200/6976, loss 0.0706\n",
            "epoch: 1, iter 1300/6976, loss 0.0720\n",
            "epoch: 1, iter 1400/6976, loss 0.0687\n",
            "epoch: 1, iter 1500/6976, loss 0.0685\n",
            "epoch: 1, iter 1600/6976, loss 0.0711\n",
            "epoch: 1, iter 1700/6976, loss 0.0723\n",
            "epoch: 1, iter 1800/6976, loss 0.0742\n",
            "epoch: 1, iter 1900/6976, loss 0.0711\n",
            "epoch: 1, iter 2000/6976, loss 0.0700\n",
            "epoch: 1, iter 2100/6976, loss 0.0734\n",
            "epoch: 1, iter 2200/6976, loss 0.0698\n",
            "epoch: 1, iter 2300/6976, loss 0.0688\n",
            "epoch: 1, iter 2400/6976, loss 0.0703\n",
            "epoch: 1, iter 2500/6976, loss 0.0712\n",
            "epoch: 1, iter 2600/6976, loss 0.0701\n",
            "epoch: 1, iter 2700/6976, loss 0.0692\n",
            "epoch: 1, iter 2800/6976, loss 0.0648\n",
            "epoch: 1, iter 2900/6976, loss 0.0733\n",
            "epoch: 1, iter 3000/6976, loss 0.0697\n",
            "epoch: 1, iter 3100/6976, loss 0.0741\n",
            "epoch: 1, iter 3200/6976, loss 0.0642\n",
            "epoch: 1, iter 3300/6976, loss 0.0711\n",
            "epoch: 1, iter 3400/6976, loss 0.0687\n",
            "epoch: 1, iter 3500/6976, loss 0.0706\n",
            "epoch: 1, iter 3600/6976, loss 0.0704\n",
            "epoch: 1, iter 3700/6976, loss 0.0685\n",
            "epoch: 1, iter 3800/6976, loss 0.0689\n",
            "epoch: 1, iter 3900/6976, loss 0.0667\n",
            "epoch: 1, iter 4000/6976, loss 0.0669\n",
            "epoch: 1, iter 4100/6976, loss 0.0720\n",
            "epoch: 1, iter 4200/6976, loss 0.0695\n",
            "epoch: 1, iter 4300/6976, loss 0.0714\n",
            "epoch: 1, iter 4400/6976, loss 0.0701\n",
            "epoch: 1, iter 4500/6976, loss 0.0649\n",
            "epoch: 1, iter 4600/6976, loss 0.0709\n",
            "epoch: 1, iter 4700/6976, loss 0.0643\n",
            "epoch: 1, iter 4800/6976, loss 0.0686\n",
            "epoch: 1, iter 4900/6976, loss 0.0692\n",
            "epoch: 1, iter 5000/6976, loss 0.0685\n",
            "epoch: 1, iter 5100/6976, loss 0.0704\n",
            "epoch: 1, iter 5200/6976, loss 0.0688\n",
            "epoch: 1, iter 5300/6976, loss 0.0674\n",
            "epoch: 1, iter 5400/6976, loss 0.0684\n",
            "epoch: 1, iter 5500/6976, loss 0.0666\n",
            "epoch: 1, iter 5600/6976, loss 0.0699\n",
            "epoch: 1, iter 5700/6976, loss 0.0680\n",
            "epoch: 1, iter 5800/6976, loss 0.0709\n",
            "epoch: 1, iter 5900/6976, loss 0.0731\n",
            "epoch: 1, iter 6000/6976, loss 0.0690\n",
            "epoch: 1, iter 6100/6976, loss 0.0678\n",
            "epoch: 1, iter 6200/6976, loss 0.0672\n",
            "epoch: 1, iter 6300/6976, loss 0.0696\n",
            "epoch: 1, iter 6400/6976, loss 0.0698\n",
            "epoch: 1, iter 6500/6976, loss 0.0721\n",
            "epoch: 1, iter 6600/6976, loss 0.0681\n",
            "epoch: 1, iter 6700/6976, loss 0.0647\n",
            "epoch: 1, iter 6800/6976, loss 0.0692\n",
            "epoch: 1, iter 6900/6976, loss 0.0678\n",
            "check results\n",
            "0.8745510017723235\n",
            "0.8775787965633177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jRdz_EQYbyp",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}